# Backend Server Configuration
PORT=3001
NODE_ENV=development
FRONTEND_URL=http://localhost:4200

# LLM Provider Configuration
LLM_PROVIDER=ollama
LLM_BASE_URL=http://localhost:11434
LLM_MODEL=llama2
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2048

# For future providers:
# OPENAI_API_KEY=your_openai_api_key_here
# CLAUDE_API_KEY=your_claude_api_key_here